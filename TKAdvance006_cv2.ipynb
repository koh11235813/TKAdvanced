{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VepJM5oP7Za"
      },
      "source": [
        "# 画像分類の実装\n",
        "\n",
        "本章では Python の代表的な画像処理のパッケージである OpenCV や Pillow を用いての基礎的な画像データの取り扱い方法について学び、その後 TensorFlow を用いて Convolutional Neural Network (以下 CNN) の実装方法を確認します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENrga9z_H-9v"
      },
      "source": [
        "## 本章の構成  \n",
        "\n",
        "- 画像処理の基礎\n",
        "- 画像のクラス分類の実装\n",
        "- CNN モデルの順伝播の流れ\n",
        "\n",
        "\n",
        "### GPU の設定\n",
        "\n",
        "本章では Colab 上の Graphics Processing Unit (以下 GPU) を用いてモデルの学習を行います。GPU を使用するために事前に下記の設定を行っておいて下さい。  \n",
        "\n",
        "1. GPU のランタイムの設定\n",
        "![GPU 設定 1](http://drive.google.com/uc?export=view&id=1wVi7zFp1vJnOFxVuKiXa0N5mDWB00hFE)\n",
        "\n",
        "![GPU 設定 2](http://drive.google.com/uc?export=view&id=1p0ftgj0bRjTgm5L6DxSar5XolowHegxj)\n",
        "\n",
        "2. ランタイムの再起動\n",
        "  - 「ランタイム」 → 「ランタイムの再起動」を選択肢、ランタイムの再起動を行います。  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddJogZV_P7Zb"
      },
      "source": [
        "## 画像処理の基礎\n",
        "\n",
        "OpenCV と Pillow という Python の画像処理パッケージを使用しての基礎的な画像の取り扱い方法について学びます。画像処理には[こちら](https://drive.google.com/file/d/1rRPd3wrXmhfk6SPyT2A_sUgy6CrtjPZ_/view?usp=sharing)の画像を使用します。リンク先の画像の上で右クリックから画像の保存を選択して下さい。  \n",
        "\n",
        "ダウンロード後、Colab にアップロードを行って下さい。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNSGlpYjP7Zc"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6eyItqlP7Zz"
      },
      "source": [
        "### Pillow の基礎\n",
        "\n",
        "Pillow は`PIL`という名前で登録されています。モジュールをインポートし、Pillow を用いて画像の読み込みましょう。また、読み込み後み `resize()` メソッドを使用して画像サイズを変更します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYIkMuPxP7Z0"
      },
      "source": [
        "# 必要なモジュールのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXrBKcqLNVdz"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Z8afhJP7Z2"
      },
      "source": [
        "# 画像の読み込み\n",
        "img = Image.open('sample.png')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAfgvgcmw8IW"
      },
      "source": [
        "Pillow を用いて読み込んだ画像は PngImageFile オブジェクトとなります。  \n",
        "PngImageFile オブジェクトは画像データであるため、Notebook 上で変数を実行すると画像を表示することが可能です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsGPJw2sw9dG"
      },
      "source": [
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcXHsolvwyBe"
      },
      "source": [
        "# サイズ変更\n",
        "img.resize((300, 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh3PSXrPP7Z5"
      },
      "source": [
        "データの型を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p5P2SaIP7Z5"
      },
      "source": [
        "type(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfPKy5swShAx"
      },
      "source": [
        "Pillow を用いて簡単な画像データ操作について学びます。  \n",
        "直接このデータ操作は後ほどの CNN の実装とは関係しませんが、基礎的な処理方法を抑えておきましょう。\n",
        "\n",
        "#### 画像の回転\n",
        "\n",
        "PngImageFile オブジェクトは様々なメソッドを持ち、簡単に画像処理を適用することができます。画像の回転には `rotate()` メソッドを使用します。引数に回転の角度を取ります。   \n",
        "CNN モデルの学習の際に使用するデータにノイズ成分を追加する際などに用います。  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nldAAIQDYUQP"
      },
      "source": [
        "# 画像の回転\n",
        "img.rotate(45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdFbu9GtYiTt"
      },
      "source": [
        "#### 画像のクロップ\n",
        "\n",
        "画像のクロップは `crop()` メソッドを使用します。引数に x 軸の座標、y 軸の座標、横幅、縦幅をとります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f01IrU6tYT_r"
      },
      "source": [
        "# 画像のクロップ\n",
        "img.crop((0, 0, 150, 150))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuCf5wZsZb-p"
      },
      "source": [
        "#### NumPy の ndarray オブジェクトに変換\n",
        "\n",
        "TensorFlow を用いてニューラルネットワークを実装する際のデータは NumPy の ndarray オブジェクトである必要がありました。  \n",
        "\n",
        "変換は単純に `np.array()` クラスを使用します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64TdN0iJZzqM"
      },
      "source": [
        "img_array = np.array(img)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvg0b7ZPZ96G"
      },
      "source": [
        "type(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q1gE9LCaA8L"
      },
      "source": [
        "データの形を確認しましょう。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIxooRHmZ8U_"
      },
      "source": [
        "img_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucsTR7MYaUnK"
      },
      "source": [
        "画像は縦幅 (hegiht) が 400 、横幅 (width) が 300 、チャンネル数 (channel) が 3 となっていることが確認できます。  \n",
        "\n",
        "それぞれのチャンネルを切り出して、それぞれが red・green・blue であることを確認しましょう。それぞれのチャンネルを切り出し、red のチャンネルを取り出す場合はその他のチャンネルの値を 0 に置き換えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPq1lFVCTr0z"
      },
      "source": [
        "img_rgb = img.convert('RGB')\n",
        "img_array = np.array(img_rgb)\n",
        "img_array[:, :, 1] *= 0 # blue チャンネルを 0 に\n",
        "img_array[:, :, 2] *= 0 # green チャンネルを 0 に\n",
        "img_red = Image.fromarray(img_array) # Pillow の型に変換"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNyTvvGwXZVR"
      },
      "source": [
        "img_red"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDhSLFYGelHX"
      },
      "source": [
        "赤色の画像が取り出すことができました。上記のコードの 0 に置き換えるチャンネルのインデックス番号を変更して、green・blue のチャンネルを抽出した場合もそれぞれ確認しておきましょう。  \n",
        "\n",
        "Pillow はこの他にも様々な画像データの操作を行うことができます。  \n",
        "詳細に関してはこちらの[公式ドキュメント](https://pillow.readthedocs.io/en/5.1.x/reference/Image.html)を確認して下さい。  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z77qI_psP7Ze"
      },
      "source": [
        "### OpenCV の基礎\n",
        "\n",
        "OpenCV は`cv2`という名前で登録されています。Pillow との使用方法は異なりますが、基本的な機能に大きな差異はありません。どちらを使用するかは実際に使用して、使いやすいと思うものを選択して下さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Zpcb_hP7Zf"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnOY47RAP7Zh"
      },
      "source": [
        "# 画像の読み込み\n",
        "img = cv2.imread('sample.png')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6jxhpVKP7Zj"
      },
      "source": [
        "OpenCV で読み込んだ画像は NumPy の ndarray 型で読み込まれます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh2HTbvHP7Zj"
      },
      "source": [
        "# 型を確認\n",
        "type(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7l3GrSgP7Zm"
      },
      "source": [
        "# サイズの確認\n",
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMxPgR4aP7Zo"
      },
      "source": [
        "# データ型の確認\n",
        "img.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmhirjQ8P7Zq"
      },
      "source": [
        "`uint8`は unsigned int の略であり、符号なし（正の値のみ）の 8 ビット整数であり、0~255 までを表現可能です。  \n",
        "\n",
        "OpenCV で読み込んだ画像は **BGR** (Blue, Green, Red) の順で格納されているため、画像の描画を行った際には、青みの強い色合いになります。  \n",
        "\n",
        "画像の描画には matplotlib の `imshow()` 関数を用います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ZplTGJP7Zt"
      },
      "source": [
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Brn-g3RP7Zv"
      },
      "source": [
        "現在 BGR の順で並んでいるチャンネルの配列を RGB に変換します。変換には `cvtColor()` 関数を用います。引数に `COLOR_BGR2RGB` を用い、BGR → RGB への変換を指定しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K58PTclHP7Zw"
      },
      "source": [
        "# BGR -> RGB\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK3ZkpEJP7Zx",
        "scrolled": false
      },
      "source": [
        "plt.imshow(img_rgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo-_xJGOP7Z-"
      },
      "source": [
        "Pillow と OpenCV の違いをまとめると下記になります。  \n",
        "\n",
        "|                  | Pillow                | OpenCV          |\n",
        "| ---------------- | --------------------- | --------------- |\n",
        "| **オブジェクト** | Pillow - PngImageFile | NumPy - ndarray |\n",
        "| **チャンネル**     | RGB                   | BGR             |\n",
        "\n",
        "Pillow と OpenCV の読み込むチャンネルの順番が異なる点には十分注意しましょう。例えば、学習は OpenCV で行い、推論の際は Pillow を使用するようなケースでは、学習時と推論時でチャンネルの順番が異なるため予測結果が望ましくない事が想定されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOL3AimuP7aO"
      },
      "source": [
        "#### グレースケール変換\n",
        "\n",
        "代表的な画像の前処理の 1 つであるグレースケール変換を施します。こちらも先程使用した `cvtColor()` 関数を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-x91DW7x2YA"
      },
      "source": [
        "# Pillow\n",
        "img = Image.open('sample.png').convert('L')\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9V45-eZP7aO"
      },
      "source": [
        "# OpenCV\n",
        "img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApjiHUlJP7aQ"
      },
      "source": [
        "matplotlib の`imshow`では、RGBが渡される規定であるため、グレースケールでは変な色合いになってしまいます。描画前に `gray()` 関数を実行しておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPZ0QRgmP7aU"
      },
      "source": [
        "plt.imshow(img_gray, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0WFNoRvP7aW"
      },
      "source": [
        "グレースケール変換を施したことによって チャンネル数が 3 から 1 になっています。データの形を確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8yKSP5-P7aX"
      },
      "source": [
        "img_gray.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag-NKfz8P7ac"
      },
      "source": [
        "#### エッジ検出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJq2nsMOP7ac"
      },
      "source": [
        "画像内の明るさの変化を検出するエッジ検出の実装を行います。エッジ検出のためのフィルタを準備します。フィルタのことを**カーネル (kernel)**とも呼ぶことも覚えておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMvcgl4XP7ad"
      },
      "source": [
        "# エッジ検出のフィルタの定義\n",
        "kernel = np.array([\n",
        "    [-1, 0, 1],\n",
        "    [-1, 0, 1],\n",
        "    [-1, 0, 1]\n",
        "])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcUYj-S2P7af"
      },
      "source": [
        "エッジ検出のフィルタを画像に適用します。フィルタを画像に適用することを畳み込み (convolution) とも呼びます。  \n",
        "畳み込みの演算は `filter2D()` 関数を用います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRGL6_PDP7af"
      },
      "source": [
        "img_conv = cv2.filter2D(img_gray, -1, kernel)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbmUOOY2P7ah"
      },
      "source": [
        "エッジ検出フィルタ適用後の画像を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruhX5PF2P7ai"
      },
      "source": [
        "plt.imshow(img_conv, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCX_lWPcP7ak"
      },
      "source": [
        "横方向に対して、エッジが検出できていることが確認できます。縦方向にもエッジ検出のフィルタを適用してみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvwzG60bP7al"
      },
      "source": [
        "kernel = np.array([\n",
        "    [-1, -1, -1],\n",
        "    [  0,  0,  0],\n",
        "    [  1,  1,  1]\n",
        "])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFRmDU0fP7an"
      },
      "source": [
        "img_conv = cv2.filter2D(img_gray, -1, kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt0TauwlP7ao"
      },
      "source": [
        "plt.imshow(img_conv, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7EWIwNZP7ap"
      },
      "source": [
        "先ほどとは異なり、縦方向に輝度の変化量が多い部分が抽出されていることがわかります。特に縦方向の輝度の変化が強い目元を確認すると横方向のエッジ検出のフィルタとの違いが確認することができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9-nhuBqLa8C"
      },
      "source": [
        "## 画像のクラス分類の実装\n",
        "\n",
        "本節では、画像データの基礎的な取り扱い方法を理解した上で、畳み込みニューラルネットワーク (Convolutional Neural Network ; 以下 CNN) の実装を行っていきます。  \n",
        "\n",
        "今回画像のクラス分類を行う問題設定は 0~9 までの 10 種類の手書き数字になります。使用するデータセットは MNIST と呼ばれるものを使用します。  \n",
        "\n",
        "![MNIST サンプル](http://drive.google.com/uc?export=view&id=1UN1f-zvpUsnJQOFeJrqSv0or_joH_Gcm)\n",
        "\n",
        "\n",
        "### データセットの準備\n",
        "\n",
        "TensorFlow を用いて、CNN を実装する際の画像のデータセットの形式を確認します。画像や自然言語などの非構造化データを取り扱う際にはまず入力値がどのような形式になっているのかを把握することが重要です。  \n",
        "\n",
        "データセットの読み込みは `tf.keras.datasets.mnist` クラスを用いて取得します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2B8X_NvPM20"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWaawi36GYos"
      },
      "source": [
        "GPU が使用可能であるか確認しましょう。  \n",
        "`name: \"/device:GPU:0\"` の表示があれば GPU が使用可能な状況となっています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocoyc0OwGUbn"
      },
      "source": [
        "# GPU が使用可能であることを確認\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUy6T0K5Oe3W"
      },
      "source": [
        "# データセットの取得\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIuGyoKvVmCf"
      },
      "source": [
        "取得したデータセットはすでに TensorFlow を用いて CNN を実装する際に適したデータ形式となっています。データセットの型、データ型、形などを確認し、どのような形式でデータセットを準備する必要があるのか確認していきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhYa7tDqWGGF"
      },
      "source": [
        "# サンプル数確認\n",
        "len(x_train), len(t_train), len(x_test), len(t_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Z4ee-9Ygw7"
      },
      "source": [
        "# サイズ確認\n",
        "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGjJp-zSXAH9"
      },
      "source": [
        "# 1 枚可視化\n",
        "img = x_train[0]\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBMI-0OcX8QK"
      },
      "source": [
        "# 目標値確認\n",
        "t_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8urnZsLY2Rk"
      },
      "source": [
        "#### TensorFlow で使用できる形式に変換\n",
        "\n",
        " 画像データの形を (height, width) から (height, width, channel) へと変換します。また画像データの値の正規化を行います。  \n",
        " 形の変換は `reshape()` メソッドに変換後の形をタプル型で引数に指定します。  \n",
        " 正規化は uint8 形式のデータの最大値である 255 で割ることで 0~1 の間に変換します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X78P1XCzabH7"
      },
      "source": [
        "x_train = x_train.reshape(60000, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(10000, 28, 28, 1) / 255.0"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD2P8Maya_Ie"
      },
      "source": [
        "# チャンネルが追加されていることを確認\n",
        "x_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETibg_xTa3Xt"
      },
      "source": [
        "# 正規化されていることを確認\n",
        "x_train[0].min(), x_train[0].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7SxZLCQ1F8o"
      },
      "source": [
        "最後に入力値は float32 のデータ型に、目標値は int32 のデータ型に変換しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO073zZy1LmT"
      },
      "source": [
        "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
        "t_train, t_test = t_train.astype('int32'), t_test.astype('int32')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFX_FqtFbIBu"
      },
      "source": [
        "### CNN のモデルの定義\n",
        "\n",
        "CNN モデルの定義を行います。まず、CNN のモデルの概要を再度確認します。\n",
        "\n",
        "![CNN モデル](http://drive.google.com/uc?export=view&id=1eDSmSKeLjU-kb-r_F_4JloJPOxrQP0AA)\n",
        "\n",
        "CNN のモデルは上図のように大きく分けて 3 つの要素からなります。説明に記載されている英字はコードと関連します。  \n",
        "\n",
        "- 特徴量抽出 : convolution + pooling\n",
        "  - 画像データからクラス分類などを行う際に使用する特徴量を抽出を行う。\n",
        "  - 畳み込み (convolution) と縮小 (pooling) を繰り返す。convolution 層を何層追加するのかなどはハイパーパラメータに該当する。\n",
        "- ベクトル化 : faltten\n",
        "  - 特徴量抽出後の値をベクトルに変換する。\n",
        "- 識別 : dense\n",
        "  - 全結合層、活性化関数を介してクラス分類を行う。  \n",
        "\n",
        "全体像を把握したところで、モデルの定義を行いましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtpZD1qBbIJB"
      },
      "source": [
        "import os, random\n",
        "\n",
        "def reset_seed(seed=0):\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlunUYaq1vr8"
      },
      "source": [
        "from tensorflow.keras import models,layers\n",
        "\n",
        "# シードの固定\n",
        "reset_seed(0)\n",
        "\n",
        "# モデルのインスタンス化\n",
        "model = models.Sequential()\n",
        "\n",
        "# モデルの構築\n",
        "## 特徴量抽出\n",
        "model.add(layers.Conv2D(filters=3, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))) # 畳み込み (convolution) 層\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2))) # pooling 層\n",
        "\n",
        "## ベクトル化\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "## 識別\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DDb2aRM5mNt"
      },
      "source": [
        "モデルの定義が完了しました。`summary()` メソッドでパラメータを確認します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA6nCGio5vrq"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf0aoxGNAxv8"
      },
      "source": [
        "1 層目の `conv2d` のパラメータの数が $30$ となっています。何故この値なのか確認します。  \n",
        "\n",
        "- カーネルのサイズ : $3\\times3$\n",
        "- 入力のチャンネル数 : $1$\n",
        "- 出力のチャンネル数 : $3$\n",
        "- 重みの数 : $(3\\times3)\\times1\\times3 = 27$\n",
        "- バイアスの数 : $3$\n",
        "- 合計のパラメータの数 : $27+3 = 30$\n",
        "\n",
        "前章で学んだ数学と同じようにパラメータ数があることが確認できました。  注意点として、今回入力値の画像は 1 チャンネルのものを使用していますが、このチャンネル数が 3 になった場合は、重みの数は 3 倍多くなります。  \n",
        "\n",
        "構造のプロットも行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7cFcZxl4jle"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xMNJmkVA4Hc"
      },
      "source": [
        "今回は非常にシンプルな CNN のモデルを定義しました。精度向上のためには、特徴量抽出の部分の convolution 層や pooling 層の数を調整したり、全結合層の層やノードの数を調整します。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1AzVn0T5lkr"
      },
      "source": [
        "### 目的関数と最適化手法の選択\n",
        "\n",
        "今回は最適化の手法に Adam を、目的関数は分類の問題設定のため sparse categorical crossentropy を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD3Oe8jI4itC"
      },
      "source": [
        "# optimizerの設定\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGKZiKr1r7b"
      },
      "source": [
        "### モデルの学習\n",
        "\n",
        "バッチサイズ、エポック数を定義して、モデルの学習を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn-tdlF2ohxk"
      },
      "source": [
        "# 学習の実行\n",
        "history = model.fit(x_train, t_train,\n",
        "                    batch_size=4096,\n",
        "                    epochs=30,\n",
        "                    validation_data=(x_test, t_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGP-a1pZNnly"
      },
      "source": [
        "今回は GPU を使用して学習を行いました。GPU のメモリの使用率は下記の `!nvidia-smi` コマンドを実行します。  \n",
        "\n",
        "`Memory-Usage` の欄を確認すると`1121MiB / 16280MiB` のように現在どの程度メモリを専有しているか確認できます。  \n",
        "\n",
        "経験的にバッチサイズはこのメモリを可能な限り使用できる大きさに調整することが多いです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN9kpKOYIuF0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65hIaLLTQHES"
      },
      "source": [
        "### 予測精度の評価\n",
        "\n",
        "学習結果を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly26-gBhQG_L"
      },
      "source": [
        "results = pd.DataFrame(history.history)\n",
        "results.tail(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk8Vdaj8QWdw"
      },
      "source": [
        "# 損失を可視化\n",
        "results[['loss', 'val_loss']].plot(title='loss')\n",
        "plt.xlabel('epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bRzZXaGQX_7"
      },
      "source": [
        "# 正解率を可視化\n",
        "results[['accuracy', 'val_accuracy']].plot(title='accuracy')\n",
        "plt.xlabel('epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpo0qVhtQG47"
      },
      "source": [
        "損失が下がり、正解率も 95% を超えており、予測精度としては悪くない事が確認できます。続いては実装の中身を分解して確認します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh08aQPGthpi"
      },
      "source": [
        "## CNN モデルの順伝播の流れ\n",
        "\n",
        "構築した CNN モデルの計算の中身を確認していきます。  \n",
        "入力画像が特徴抽出からベクトル化にかけてどのように変化しているのかを簡単に確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTkS5pK1othz"
      },
      "source": [
        "# 推論に使用するデータを切り出し + バッチサイズの追加\n",
        "x_new = np.array([x_train[0]])\n",
        "x_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5FlVP30o8VX"
      },
      "source": [
        "学習済みモデルの層は `layers` 属性から取得することができ、層のインデックス番号を使用すると特定の層の取り出しを行うことが可能です。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyzlN3ndpzBh"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26MCzH4MqJMt"
      },
      "source": [
        "切り出した重みの取得には `get_weights()` メソッドを用います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr8c1hMHp5Cp"
      },
      "source": [
        "model.layers[0].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St0CvSs3qQKw"
      },
      "source": [
        "### convolution 層の計算\n",
        "\n",
        "切り出した層に値を渡すことによって計算を行うことができます。1 層目の convolution 層の計算を実行し、出力データを画像として可視化してみましょう。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjLCzG5OYa_u"
      },
      "source": [
        "output = model.layers[0](x_new) # convolution 層の計算\n",
        "output = output[0].numpy() # NumPy の ndarray オブジェクトに変換"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XYFr1eUqsqc"
      },
      "source": [
        "今回の convolution 層のフィルタの数は 3 でした。そのため、出力されるデータのチャンネル数は 3 になります。それぞれのチャンネル毎に可視化を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaB_SiW0wIZe"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7ZQ_tOKm6Zl"
      },
      "source": [
        "# 1 つ目の出力\n",
        "plt.imshow(output[:, :, 0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G2qniq1e3ji"
      },
      "source": [
        "# 2 つ目の出力\n",
        "plt.imshow(output[:, :, 1], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK8khW69Ya7B"
      },
      "source": [
        "# 3 つ目の出力\n",
        "plt.imshow(output[:, :, 2], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrZWLmQiuB1C"
      },
      "source": [
        "それぞれ個別のフィルタが適用され、異なる出力が確認できます。この画像から人間側がどのような特徴を抽出しているか理解することは少し困難ですが、前章で学んだ数学の処理が施されている事が確認できます。\n",
        "\n",
        "### pooling 層の計算\n",
        "\n",
        "pooling 層の計算を確認します。pooling サイズが 2x2 だったため、出力のサイズは 1/2 になります。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-WG4tWeYa1p"
      },
      "source": [
        "output = model.layers[0](x_new) # convolution 層の計算\n",
        "output = model.layers[1](output) # pooling 層の計算（サイズを 1/2 に変換）\n",
        "output = output[0].numpy()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XomaABYfYawJ"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbhOMj5UYaq-"
      },
      "source": [
        "# 1 つ目の出力\n",
        "plt.imshow(output[:, :, 0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSVVyJ7I0jwl"
      },
      "source": [
        "# ２つ目の出力\n",
        "plt.imshow(output[:, :, 1], cmap='gray');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiVdQrpT0ryh"
      },
      "source": [
        "# 3つ目の出力\n",
        "plt.imshow(output[:, :, 2], cmap='gray');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55FtyUxmwRcJ"
      },
      "source": [
        "### ベクトル化\n",
        "\n",
        "先程の出力の形は 13, 13, 3 になります。全ての値の数の合計は $13\\times13\\times3 = 507$ となります。実際に 507 次元のベクトルに変換されていることを確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDWuS2cqYamW"
      },
      "source": [
        "output = model.layers[0](x_new) # convolution 層の計算\n",
        "output = model.layers[1](output) # pooling 層の計算（サイズを 1/2 に変換）\n",
        "output = model.layers[2](output) # ベクトル化\n",
        "output = output[0].numpy()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGQfeO_-YagS"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHQxIRjFW5Xg"
      },
      "source": [
        "## 練習問題 本章のまとめ\n",
        "\n",
        "本章で学んだ内容を復習しましょう。下記の内容を次のセルに記述し、実行結果を確認してください。（必要に応じてセルの追加を行ってください。）  \n",
        "\n",
        "CNN モデルのハイパーパラメータ調整を行い、予測精度にどのような変化があるのか確認して下さい。  \n",
        "\n",
        "*ハイパーパラメータ調整のポイント*\n",
        "\n",
        "- convolution 層の数\n",
        "- カーネルサイズ\n",
        "- パディング\n",
        "- pooling 層の数\n",
        "- pooling のサイズ\n",
        "- バッチノーマリゼーション層の追加\n",
        "- 全結合層のノード・層の数\n",
        "- 最適化手法\n",
        "\n",
        "*発展*  \n",
        "- 学習済みモデルの convolution 層を切り出し、計算を行い、出力結果を確認して下さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ4tI2vwXWbs"
      },
      "source": [
        "# モデルの定義\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKP5X3jPWYF3"
      },
      "source": [
        "# 目的関数と最適化手法の選択\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkcfUQYHYPmA"
      },
      "source": [
        "# モデルの学習\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Rte86TYRZj"
      },
      "source": [
        "# 予測精度の評価\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeIS9e3SxXxJ"
      },
      "source": [
        "# convolution 層の出力の確認\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOVvDcE-H_Bb"
      },
      "source": [
        "---\n",
        "© 株式会社キカガク及び国立大学法人 豊橋技術科学大学"
      ]
    }
  ]
}