{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TKAdvance003_nn3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9acRIqWsqkel"
      },
      "source": [
        "# ニューラルネットワークの実装 1 （分類）\n",
        "\n",
        "本章では、数学の章で学んだニューラルネットワークの実装をオープンソースフレームワークである **TensorFlow** を使ってを行います。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB_H52dN1Hap"
      },
      "source": [
        "## 本章の構成\n",
        "\n",
        "- TensorFlow の基礎\n",
        "- TensorFlow による分類モデルの学習\n",
        "- 学習済みモデルの保存と推論"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcbUTK1N1J0C"
      },
      "source": [
        "## TensorFlow の基礎\n",
        "\n",
        "![TensorFlow](http://drive.google.com/uc?export=view&id=1Bnf_KGHYGRGzlirq6WHP_T2VQZ0ACM9J)\n",
        "\n",
        "*TensorFlow, the TensorFlow logo and any related marks are trademarks of Google Inc.*\n",
        "\n",
        "[TensorFlow](https://www.tensorflow.org/) とは、Google 社開発している機械学習のためのエンドツーエンドのオープンソースプラットフォームです。ソースコードは[こちら](https://github.com/tensorflow/tensorflow)から確認することができます。    \n",
        "\n",
        "### TensorFlow の特徴\n",
        "\n",
        "TensorFlow の特徴には下記の 3 点が挙げられます。  \n",
        "\n",
        "- 初心者にも使いやすいインターフェースで作られている\n",
        "- ユーザー数が多いため、世界中の人からの情報が集まるコミュニティがある\n",
        "- エッジデバイスへの連携や分散処理、学習可視化ソフトなどモデル作成以外の部分も包括的なエコシステムがある\n",
        "\n",
        "こういった理由からも、初学者にとっておすすめのフレームワークになります。まずは TensorFlow で大枠を掴んだあとに PyTorch などの他のフレームワークを使用することをおすすめします。    \n",
        "\n",
        "\n",
        "### Keras とは\n",
        "\n",
        "[Keras](https://www.tensorflow.org/guide/keras) とは TensorFlow でディープラーニングのモデルの構築及び学習を行うことが可能な高レベルな API です。Keras には大きく 3 種類の API （実装方法）が存在します。  \n",
        "\n",
        "- [Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) \n",
        "- [Functional API](https://www.tensorflow.org/guide/keras/functional)\n",
        "- [Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
        "\n",
        "Sequential API は比較的単純なモデルの構築を簡単かつ、短いコードで実装を行うことが可能です。本講座では、主にこの Sequetial API を用いてディープラーニングのモデルの構築を行います。  \n",
        "Functional API や Subclassing API を使うと、複雑なモデルの構築も可能です。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agl8tSFrVKpP"
      },
      "source": [
        "### TensorFlow の読み込み\n",
        "\n",
        "TensorFlow は Google Colaboratory 上で既にインストールされているため、そのまま読み込む事が可能です。TensorFlow は読み込む際に `tf` として省略することが慣例になっています。  \n",
        "\n",
        "また、Colab ではデフォルトではバージョン 1.x 系が読み込まれる設定となっています。2020 年 2 月現在では 2.1.0 が最新となっており、2.x 系がこれからのデフォルトに変わっていきますので本講座では 2.x 系を使っていきます。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mkXaJ7L1otL"
      },
      "source": [
        "# ライブラリの読み込み\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s94q4cXP2p4g"
      },
      "source": [
        "#　バージョンの確認\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOzFUX_u3nGh"
      },
      "source": [
        "TensorFlow を扱っていく上で、モデル構築には TensorFlow 内部に組み込まれている Keras を使用して実装を行っていきます。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBaAVvGIdyPp"
      },
      "source": [
        "## TensorFlow による分類モデルの学習\n",
        "\n",
        "ニューラルネットワークのモデル構築・学習を行います。流れは下記の 5 点を意識します。  \n",
        "\n",
        "1. データセットの準備\n",
        "2. ネットワークの構築\n",
        "3. 目的関数・最適化手法の選択\n",
        "4. ネットワークの学習\n",
        "5. 予測精度の評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyF4ATKhMh37"
      },
      "source": [
        "### データセットの準備\n",
        "\n",
        "今回は scikit-learn に準備されている[乳がんに関するデータセット](https://goo.gl/U2Uwz2)を用い、クラス分類を通して実装方法を学びます。  \n",
        "今回使用するデータセットの目的変数には、乳がんが陽性 (0)　もしくは、陰性 (1) の 2 種類の値を持ちます。また、入力変数には乳がんの検査で採取された細胞に関する情報を持ちます。  \n",
        "\n",
        "入力変数の詳細に関しては[こちら](https://goo.gl/U2Uwz2)を参照して下さい。\n",
        "\n",
        "（scikit-learn ではデータ取得時に NumPy の ndarray オブジェクトでデータが保存されていますが、今回 csv ファイルから読み込んだデータを学習に使用することを想定し、一度 Pandas の DataFrame オブジェクトに変換してから実装を行います。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrgfy-Kc4X8L"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwk57ouDYBUO"
      },
      "source": [
        "# データセットの読み込み\n",
        "dataset = load_breast_cancer()\n",
        "colms_name = dataset.feature_names\n",
        "x = dataset.data\n",
        "t = dataset.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGDUrny-Xpji"
      },
      "source": [
        "# 読み込んだデータセットをデータフレームに変換\n",
        "df = pd.DataFrame(data=x, columns=colms_name)\n",
        "df['Target'] = t\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgqglZpwlhSD"
      },
      "source": [
        "Target の列が目的変数、その他の列を入力変数とし、乳がんが陽性 / 陰性の 2 値の分類を行います。  \n",
        "データセットの中身を簡単に確認しておきます。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud3PSxI6tIO6"
      },
      "source": [
        "# サンプル数（行数）、入力・目的変数の数（列数）の確認\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JbAqxEQu43J"
      },
      "source": [
        "今回はサンプル数が 569 、入力変数が 30 、目的変数の数が 1 であることが確認できます。  \n",
        "この**入力変数の数はニューラルネットワークの入力層のノードの数となります。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rHpbBLStNjv"
      },
      "source": [
        "# 分類するクラスの種類の確認\n",
        "df['Target'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GyO2CsfvHOV"
      },
      "source": [
        "目的変数の種類は 0 ~ 1 の 2 種類が存在します。このようにまずデータセットの全体像を理解する癖を付けるようにしておきましょう。  \n",
        "\n",
        "**2 種類の分類になるため、ニューラルネットワークの出力層のノードの数は 2 になります。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFQlfdwBviIt"
      },
      "source": [
        "#### 入力変数と目的変数の切り分け\n",
        "\n",
        "TensorFlow を用いて、ニューラルネットワークの学習を実装する際も scikit-learn を用いての機械学習アルゴリズムの実装と同様にデータセットの入力変数と目的変数の切り分けを行う必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PNYeOYUln-2"
      },
      "source": [
        "# 入力変数（Target 列以外の列を取得）\n",
        "x = df.drop('Target', axis=1)\n",
        "\n",
        "# 目的変数（Target の列のみを取得）\n",
        "t = df['Target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnUj0WN8sje5"
      },
      "source": [
        "正しく切り分けられているかデータの中身を表示して確認しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXlT9bDkvMK5"
      },
      "source": [
        "x.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cak4uvoEv7VL"
      },
      "source": [
        "#### TensorFlow で計算できるデータの形式に変換\n",
        "\n",
        "TensorFlow で計算を行うためには、下記の 2 点を満たしている必要があります。こちらが指定された形式となっていない場合、学習の際にエラーが出てしまいます。  \n",
        "\n",
        "- 入力変数や目的変数が NumPy の ndarray オブジェクトである\n",
        "- 分類の場合、ラベルの最小値が 0 である"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bk9XFK3wm9z"
      },
      "source": [
        "**NumPy の ndarray オブジェクトの取得**\n",
        "\n",
        "まずは、切り出したデータセットの型を確認しましょう。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tYvLqQqwy90"
      },
      "source": [
        "type(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-vPNbL9wepd"
      },
      "source": [
        "今回は Pandas でデータセットの読み込みを行ったため、Pandas の DataFrame オブジェクトであることが確認できます。  \n",
        "\n",
        "この DataFrame 型は　`.values`　という属性の中に NumPy の ndarray オブジェクトを持ちます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ndqaf-5Lgw"
      },
      "source": [
        "x = x.values\n",
        "t = t.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMITy-aJxFEc"
      },
      "source": [
        "type(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6KpS_qJ4LW7"
      },
      "source": [
        "TensorFlow の入力値のデータ型は `numpy.float32`、分類問題では目標値のデータ型は `numpy.int32` になっている事が一般的です。 データセットのデータ型の変更を行います。 numpy.ndarray は astype() メソッドを使ってデータ型を変更することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sc9Xy9H5WOu"
      },
      "source": [
        "x.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpawtG5q5Y7e"
      },
      "source": [
        "# それぞれのデータ型を変換\n",
        "x = x.astype('float32')\n",
        "t = t.astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTAFBe4D5fNL"
      },
      "source": [
        "x.dtype, t.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3NgOLMcxGSl"
      },
      "source": [
        "**分類の場合、ラベルの最小値が 0 である**\n",
        "\n",
        "ラベルの最小値が 0 になっているかは、NumPy の `unique` 関数を用いると重複のないユニーク（固有）の値を取り出す事ができます。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOBYwm4Iyddb"
      },
      "source": [
        "np.unique(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTHbXH1zyfYF"
      },
      "source": [
        "今回はラベルが 0 から始まっているので特に変更を加える必要がありません。  \n",
        "\n",
        "実際には、ラベルが製品名などでついている場合が多いでしょう。その場合は、ラベルを 0 から順につけて行きかつ、そのラベルと製品名が一致するようにデータセットの準備を行う必要があります。  \n",
        "\n",
        "これでニューラルネットワークの学習に使用するためのデータセットの準備が整いました。最後にそれぞれのデータセットの中身を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pTLPnfezOz1"
      },
      "source": [
        "t[:10], type(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOj19w3izQ2v"
      },
      "source": [
        "x[:5], type(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq-tTaTdzRth"
      },
      "source": [
        "#### 学習用データセットとテスト用データセットに分割\n",
        "\n",
        "基礎編で学んだように基本的に機械学習のモデルを学習させる際にはデータセットの分割を行う必要があります。分割するデータセットの種類を確認します。  \n",
        "\n",
        "| 名称                      | 使用目的                                                     |\n",
        "  | ------------------------------- | ------------------------------------------------------------ |\n",
        "  | 学習用データセット (train)      | モデルを学習させるためのデータセット                         |\n",
        "  | 検証用データセット (validation) | ハイパーパラメータの調整が適切なのか検証するためのデータセット |\n",
        "  | テスト用データセット (test)     | 学習済みモデルの性能を評価するためのデータセット             |\n",
        "\n",
        "実際には上記のように 3 種類のデータセットに分割することが望ましいですが、今回は学習用データセット (train) とテスト用データセット (test) の 2 つに分割を行い、ニューラルネットワークの学習・テストを行います。またこのようにデータセットの分割を行い検証を行う方法を[ホールドアウト法](https://ja.wikipedia.org/wiki/%E4%BA%A4%E5%B7%AE%E6%A4%9C%E8%A8%BC)と呼びました。  \n",
        "\n",
        "分割には scikit-learn に用意されている `model_selection.train_test_split` を使用します。今回は 学習用データセットを 70%、テスト用データセットを 30% とします。  \n",
        "\n",
        "ここで **再現性の確保** のために引数の `random_state` の値の設定を忘れないようにしましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj2eyqBDzm6c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 学習用データセットとテスト用データセットの分割\n",
        "x_train, x_test, t_train, t_test = train_test_split(x, t, train_size=0.7, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzIAYNtx359q"
      },
      "source": [
        "分割が完了した後は、それぞれのデータセットのサンプル数を確認し、正常に分割できていることを確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4ZjsULo2Mqm"
      },
      "source": [
        "x_train.shape, x_train.dtype, x_test.shape, x_test.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i02JXEAr2Oqe"
      },
      "source": [
        "t_train.shape, t_train.dtype, t_test.shape, t_test.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6mCj1XD3iNb"
      },
      "source": [
        "### モデルの定義\n",
        "\n",
        "モデルの定義では、最もシンプルな `tf.keras.Sequential` クラスを使用します。使い方はシンプルで以下の通りです。  \n",
        "\n",
        "1. `Sequential()` でインスタンス化\n",
        "2. `layers.層の名前()` で層を定義し、 `add` メソッドでネットワークに追加\n",
        "\n",
        "この `tf.keras.layers.層の名前()` の部分で全結合層 (`Dense layer`) や活性化関数 (`Activation layer`)、バッチ正規化 (`BatchNormalization layer`) などを追加することができます。    \n",
        "\n",
        "モデルの定義の前に、乱数のシードの固定も行いましょう。Python 、`random` 関数、 `numpy` そして `tensorflow` それぞのシードの固定を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pD6rI-2KutA"
      },
      "source": [
        "import os, random\n",
        "\n",
        "def reset_seed(seed=0):\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    random.seed(seed) # random関数のシードを固定\n",
        "    np.random.seed(seed) # numpyのシードを固定\n",
        "    tf.random.set_seed(seed) # tensorflowのシードを固定"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiG3tYZ-Q2Kt"
      },
      "source": [
        "from tensorflow.keras import models,layers\n",
        "\n",
        "# シードの固定\n",
        "reset_seed(0)\n",
        "\n",
        "# モデルのインスタンス化\n",
        "model = models.Sequential()\n",
        "\n",
        "# モデルの定義（層の追加）\n",
        "model.add(layers.Dense(units=10, input_shape=(30,))) # 入力層\n",
        "model.add(layers.Activation('relu')) # 活性関数 (ReLU)\n",
        "model.add(layers.Dense(units=2, input_shape=(10,))) # 出力層\n",
        "model.add(layers.Activation('softmax')) # ソフトマックス関数（値を 0~1 に変換）"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqMZAVUFyuc"
      },
      "source": [
        "上記のコードでは下記のようなネットワークの構築を行っています。図を確認し、全体像を掴みましょう。    \n",
        "\n",
        "![定義したネットワークの構造](http://drive.google.com/uc?export=view&id=1k5Wxxhe-2h8cWjV1auKoL4XX05peXd9t)\n",
        "\n",
        "コードの詳細を確認します。  \n",
        "\n",
        "- `reset_seed(0)` \n",
        "  - 先程定義した関数を使用し、シードの固定を行います。ニューラルネットワークの重みはランダムに決定されるため、この乱数のシードを固定することにより再現性を確保します。  \n",
        "\n",
        "- `models.Sequential()` クラス\n",
        "  - Sequential モデルのインスタンス化を行います。ニューラルネットワークの定義を行うための箱を準備しているイメージを持つと理解しやすいです。インスタンス化したものを `model` という変数に格納しています。    \n",
        "\n",
        "- `add()` メソッド\n",
        "  - 全結合層や活性化関数の層の追加を行います。引数に後述の `layers` モジュールをとります。  \n",
        "\n",
        "- `layers.Dense()` クラス\n",
        "  - 全結合層の定義を行うことができます。`units` の引数は出力するノードの数を指定します。この値はハイパーパラメータに該当するため調整が必要な部分になります。  \n",
        "  - `input_shape` の引数では、入力のノード数を指定します。入力層では、入力変数の数を指定します。（引数はフレームワーク側で自動で算出される値になるため、指定は必須ではありません。）  \n",
        "その他の引数などに関してはこちらの[公式ドキュメント](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)を参照して下さい。\n",
        "\n",
        "- `layers.Activation()` クラス\n",
        "  - 活性化関数の定義を行うことができます。引数に活性化関数の名前を指定することで定義できます。指定可能な活性化関数の種類に関してはこちらの[公式ドキュメント](https://www.tensorflow.org/api_docs/python/tf/keras/activations)を参照して下さい。\n",
        "  - 今回は入力層から中間層の全結合層の後に ReLU 関数で非線形変換を行い、ネットワークの最後にソフトマックス関数を適用し、出力された値の合計が 1 になるように変換を行っています。  \n",
        "\n",
        "`layers` モジュールには様々な種類の層のクラスが存在します。詳細はこちらの[公式ドキュメント](https://www.tensorflow.org/api_docs/python/tf/keras/layers)を確認して下さい。  \n",
        "\n",
        "\n",
        "上記が基本的な書き方の 1 つですが、もう少し改善することができます。  \n",
        "活性化関数ですが、Activation 層で指定する代わりに、Dense 層の中の `activation` という引数で指定しても同じ結果が得られます。また入力変数の数は一番最初に指定した後は、フレームワーク側で吸収してくれるため入力をしなくても構いません。今回はノードの数を簡単に把握することができるため恩恵は少ないですが、画像処理向けのニューラルネットワークである CNN(Convolutional Neural Network) などではこの機能が活躍します。  \n",
        "\n",
        "また、入力層と出力層の値は問題設定（使用するデータセット）によって決定されるため、それぞれの値を変数に格納して使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epGxvEpisE_3"
      },
      "source": [
        "# 入力・出力層のノードの数を定義\n",
        "n_input, n_output = len(x_train[0]), len(np.unique(t_train))\n",
        "n_input, n_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxS4rm70Fypd"
      },
      "source": [
        "# シードの固定\n",
        "reset_seed(0)\n",
        "\n",
        "# モデルのインスタンス化\n",
        "model = models.Sequential()\n",
        "\n",
        "# モデルの構築\n",
        "model.add(layers.Dense(10, activation='relu', input_shape=(n_input,)))\n",
        "model.add(layers.Dense(n_output, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhHh-QoUFyjn"
      },
      "source": [
        "#### モデルの可視化\n",
        "\n",
        "これでモデルの定義が完了しました。定義したモデルの確認を行います。`summary()` メソッドでモデル内の層やパラメータの数を確認することが可能です。  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-geXaZjFybv"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMZbq9a6IHV4"
      },
      "source": [
        "また、`plot_model` を使用することにより、順伝播の流れを可視化することが可能です。<sub>*注 1<sub>\n",
        "\n",
        "*注1 : 下記のコマンドは Google Colaboratory 上では正常に動作が確認されていますが、ローカル環境で実行する際には [pydot](https://pypi.org/project/pydot/) と [graphviz](https://pypi.org/project/graphviz/) というパッケージが必要になります。*  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzcDXgz7IH9K"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKyUFXPmQ2Kv"
      },
      "source": [
        "### 目的関数・最適化手法の選択\n",
        "\n",
        "学習の際に必要となる目的関数や最適化手法などの選択を行います。  \n",
        "それぞれ `model.compile` メソッドを用いて設定を行うことができます。引数を確認しましょう。\n",
        "\n",
        "- `optimizer` : 最適化手法の指定\n",
        "  - `'adam'` 、`'sgd'` などの最適化手法の名前の文字列を指定するか、`tf.keras.optimizers` 内のクラスのインスタンスを使用します。\n",
        "- `loss` : 目的関数の指定\n",
        "  - 回帰問題には平均ニ乗誤差 `MSE` (MeanSquaredError) 、分類問題には `SparseCategoricalCrossentropy` または、`BinaryCrossentropy` の文字列を指定するか、`tf.keras.losses` モジュール内のクラスを使用します。\n",
        "- `metrics`：学習中に監視する指標の指定\n",
        "  - 分類問題であれば、`accuracy` などの文字列を指定するか、`tf.keras.metrics` モジュール内のクラスを使用します。\n",
        "\n",
        "詳細の設定方法に関してはこちらの[公式ドキュメント](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)を参照して下さい。\n",
        "\n",
        "今回は目的関数には 2 クラス分類のため、`SparseCategoricalCrossentropy` を使用し、 最適化手法に `SGD` を、監視する評価指標には `accuracy` として正解率を指定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhpy0uCwQ2Kv"
      },
      "source": [
        "# モデルのコンパイル\n",
        "model.compile(optimizer='sgd',\n",
        "               loss='sparse_categorical_crossentropy',\n",
        "               metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dRw7FXsYM0L"
      },
      "source": [
        "学習係数の設定を行う場合には、下記のように `tf.keras.optimizers` 内のクラスをインスタンス化して、ネットワークのコンパイル時に渡します。  \n",
        "\n",
        "```python\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "               loss='sparse_categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knryxKqgYkVp"
      },
      "source": [
        "### モデルの学習\n",
        "\n",
        "モデルの構築、目的関数・最適化手法の指定、予測精度の評価方法などの学習の準備が整いました。  \n",
        "\n",
        "学習を実行するには `model.fit` メソッドを実行します。重要な引数は以下の 3 点です。\n",
        "\n",
        "- `epochs`：入力データ全体に対する 1 つの反復回数を指定します。\n",
        "- `batch_size`：データをバッチにスライスし、バッチごとに反復処理されます。その各バッチのサイズを指定します。\n",
        "- `validation_data`：テスト用データセットに対する予測精度を監視するため、`(入力値, 目標値)` のタプルを渡します。  \n",
        "\n",
        "引数からわかるようにここでは、ミニバッチ学習に関する設定を指定しています。バッチサイズを 10 、エポック数を 30 と指定して一度実行してみましょう。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOaLur4l9bpI"
      },
      "source": [
        "# モデルの学習\n",
        "history = model.fit(x_train, t_train,\n",
        "          batch_size=10,\n",
        "          epochs=30,\n",
        "          validation_data=(x_test, t_test),\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0meU0go95sp"
      },
      "source": [
        "学習が完了しました。モデルの定義を行う際に定義した変数 `model` には学習済みの重みが格納されている状態になっています。  \n",
        "\n",
        "また、変数 `history` には学習結果が格納されています。\n",
        "\n",
        "\n",
        "### 予測精度の評価\n",
        "\n",
        "学習の経過を可視化してみましょう。学習結果が格納されている`history` 変数は `history` 属性を持ち、辞書型で結果を取得することができます。  \n",
        "\n",
        "下記のコードでは辞書型で取得した値を Pandas の DataFrame オブジェクトに変換し、プロットします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx9hwskV-RYH"
      },
      "source": [
        "# 学習結果を取得\n",
        "results = pd.DataFrame(history.history)\n",
        "results.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmMELaUK_H8l"
      },
      "source": [
        "x 軸に epoch 数をとり、 y 軸の値はそれぞれの epoch 時の損失・正解率をとります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBZ_jAoCfIVc"
      },
      "source": [
        "# 損失を可視化\n",
        "results[['loss', 'val_loss']].plot(title='loss')\n",
        "plt.xlabel('epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scle5Rsai_Um"
      },
      "source": [
        "# 正解率を可視化\n",
        "results[['accuracy', 'val_accuracy']].plot(title='metric')\n",
        "plt.xlabel('epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnUUBbyYPoXt"
      },
      "source": [
        "損失・正解率ともに横ばいとなっているため、うまく学習が進んでいない事が確認できます。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7JvCVQyQ4dU"
      },
      "source": [
        "### 精度向上のための工夫\n",
        "\n",
        "学習は正常に行うことができるようになりましたが、ここからが予測精度向上のための試行錯誤の段階になります。  \n",
        "\n",
        "1 つの予測精度を向上させる方法として、バッチノーマリゼーション (BatchNormalization) が挙げられます。  \n",
        "\n",
        "バッチノーマリゼーションでは、ミニバッチごとに平均 $\\bar x$ と 標準偏差 $\\sigma$ を求め、\n",
        "$$\n",
        "\\begin{align}\n",
        "x_s &= \\frac{x-\\bar x}{\\sigma} \\\\\\\\\n",
        "\\hat x &= \\alpha x_s + \\beta\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "のように $\\hat x$ へと各変数ごとに変換を行います。ここで、$\\alpha$ と $\\beta$ はパラメータであり、単純な正規化のように平均 0、標準偏差 1 とするのではなく、平均 $\\beta$、標準偏差 $\\alpha$ となるように変換を行います。必ずしも平均 0、標準偏差 1 が良いとは限らないためです。  \n",
        "\n",
        "実装としては、各バッチ毎に平均と標準偏差を定めて標準化を行うといった非常に簡単な手法なのですが、こちらを層に加えることで各変数感のスケールによる差を吸収できます。  \n",
        "\n",
        "それでは、バッチノーマリゼーションをモデルに追加しましょう。`layers` モジュール内の `BatchNormalization()` クラスを使用します。 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGbp_aT_SmVT"
      },
      "source": [
        "#  シードの固定\n",
        "reset_seed(0)\n",
        "\n",
        "# モデルのインスタンス化\n",
        "model = models.Sequential()\n",
        "\n",
        "# モデルの構築\n",
        "model.add(layers.BatchNormalization(input_shape=(n_input,))) # バッチノーマリゼーションの層を追加\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(n_output, activation='softmax'))\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer='SGD',\n",
        "               loss='sparse_categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "# モデルの学習\n",
        "history = model.fit(x_train, t_train,\n",
        "          batch_size=10,\n",
        "          epochs=30,\n",
        "          validation_data=(x_test, t_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQPAS9Os--j4"
      },
      "source": [
        "先程と同様に学習結果をプロットし、確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwYkTcrPmNqt"
      },
      "source": [
        "# 学習結果を取得\n",
        "results = pd.DataFrame(history.history)\n",
        "# 損失を可視化\n",
        "results[['loss', 'val_loss']].plot(title='loss')\n",
        "plt.xlabel('epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZdv_fBLmNqw"
      },
      "source": [
        "# 正解率を可視化\n",
        "results[['accuracy', 'val_accuracy']].plot(title='metric')\n",
        "plt.xlabel('epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN4_sNuqTcwV"
      },
      "source": [
        "飛躍的に正解率が向上しました。正解率の値が良くなっていれば成功です。  \n",
        "\n",
        "このようにディープラーニングでは、BatchNormalization を含めた細かな精度向上のポイントがあるため、今後も調べながら進めていきましょう。TensorFlow では、多くの機能がすでに実装されているため、上記のコードのように少し付け加えるだけでその効果を検証できるため、非常に便利です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99H2StcpT1P2"
      },
      "source": [
        "## 学習済みモデルの保存と推論\n",
        "\n",
        "学習済みモデルは変数 `model` に格納されています。この学習済みモデルのファイルの書き出し、書き出したモデルを読み込み推論する方法を確認します。  \n",
        "\n",
        "モデルの保存には `save()` メソッドを使用します。引数に出力先のファイルパスを指定します。  \n",
        "（Colab 上で保存する場合、下記のようにファイルパスを `/content` から始める必要があります。）  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UQ6vnvJZ6qR"
      },
      "source": [
        "# モデルの保存\n",
        "filepath = '/content/saved_model.h5'\n",
        "model.save(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TNuXefPfBDj"
      },
      "source": [
        "保存されているファイルを確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmJenScyCF6h"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfKOrChUf7Z7"
      },
      "source": [
        "### 学習済みモデルをロード\n",
        "\n",
        "学習済みモデルは `keras.models.load_model(filepath)` を用いて再インスタンス化できます。  \n",
        "モデルがすでに一度コンパイルされている場合、`load_model` は、学習時の設定を利用して、コンパイルを行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vjgdSQDblQ_"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# モデルの読み込み\n",
        "loaded_model = load_model(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwP16G9bri7"
      },
      "source": [
        "### 予測値の計算\n",
        "\n",
        "本来であれば学習を終えたモデルは新しく取得したデータに対して推論しますが、今回は試しに学習で使用したデータセットの最初のサンプルに対する予測値を計算してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJsyp-GIgP6Y"
      },
      "source": [
        "x_new = np.array(x_train[0], dtype=np.float32)\n",
        "x_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFwoAw2VEwXS"
      },
      "source": [
        "分類のモデルでクラスを推論する場合は `predict()` メソッドを使用します。推論結果はそれぞれのクラスに対する確率になります。推論結果がどちらのクラスなのか確認する際には `argmax()` メソッドを使用します。（値の大きいものを取得することができるメソッドになります。）  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9By-vGYKgVU4"
      },
      "source": [
        "# 予測値の計算\n",
        "loaded_model.predict(x_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csFRnLWqgatg"
      },
      "source": [
        "推論で使用する際には、`(バッチサイズ, 入力変数の数)` という形式となっていないとエラーが起きます。今回であれば、`(1, 10)` が望ましいデータの形と言えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrZnGksKK2k_"
      },
      "source": [
        "x_new = x_new[tf.newaxis]\n",
        "x_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRyHgZLrjHhf"
      },
      "source": [
        "# 確率の計算\n",
        "y_proba = loaded_model.predict(x_new)\n",
        "y_proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcXcxnpRFT90"
      },
      "source": [
        "# 予測値の計算\n",
        "y_pred = y_proba.argmax(axis=-1)\n",
        "y_pred[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uGG0gFKWXH6"
      },
      "source": [
        "正常に予測できている事が確認できました。  \n",
        "最後に対応する目標値を確認し、推論結果が正解しているか確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFyJ1RdtEaqb"
      },
      "source": [
        "# 目標値の確認\n",
        "t_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ytrPvVKfwv"
      },
      "source": [
        "予測値と目標値が一致しており、うまく推論できている事が確認できました。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urz2OjQaF8PS"
      },
      "source": [
        "## 練習問題 本章のまとめ\n",
        "\n",
        "本章で学んだ内容を復習しましょう。下記の内容を次のセルに記述し、実行結果を確認してください。（必要に応じてセルの追加を行ってください。）\n",
        "\n",
        "こちらの[ワインの等級に関するデータセット](https://drive.google.com/file/d/1InIHnFv9N-GoKt2cgAnGD1DqX2TUyrnn/view?usp=sharing)を使用して、ワインの等級の分類を行うニューラルネットワークの構築を行って下さい。テスト用データセットに対し、80% の正解率を目指しましょう。    \n",
        "実装の手順は下記を参照して下さい。  \n",
        "\n",
        "- データセットのアップロード\n",
        "- データセットの準備\n",
        "  - 入力変数と目的変数の切り分け\n",
        "  - TensorFlow で計算できるデータの形式に変換\n",
        "  - 学習用データセットとテスト用データセットの切り分け\n",
        "- モデルの定義\n",
        "- 目的関数・最適化手法の選択\n",
        "- モデルの学習\n",
        "- 予測精度の評価\n",
        "- ハイパーパラメータの調整\n",
        "- 学習済みモデルの保存と推論\n",
        "\n",
        "*補足*  \n",
        "- 目的変数は `Class` 列になります。入力変数にはその他の列を全て使用して下さい。  \n",
        "- データセットの切り分けはテスト用データセットが全体の 30% となるように行って下さい。  \n",
        "- ハイパーパラメータの調整では、層・ノードの数や活性化関数、バッチノーマリゼーションなど学んだ知識を生かして、試行錯誤を行って下さい。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghbCaLzLn8kN"
      },
      "source": [
        "#### データセットの準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1kQxtrDnDGN"
      },
      "source": [
        "# データセットのアップロード\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKFXIRs_nDGV"
      },
      "source": [
        "df = pd.read_csv('wine_class.csv')\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMafD8r0nEZg"
      },
      "source": [
        "# 入力変数と目的変数の切り分け\n",
        "x = df.drop(['Class'], axis=1).values\n",
        "t = df['Class'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7TQUhtHnEgd"
      },
      "source": [
        "# それぞれの形、データ型の確認\n",
        "print(x.shape, t.shape)\n",
        "print(x.dtype, t.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n19DKDYQnEsi"
      },
      "source": [
        "# 目標値の最小値が 0 であることを確認\n",
        "np.unique(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUr0ce0Kuys3"
      },
      "source": [
        "今回は目標値が 1 から始まっているため、0 から始まるように処理を加えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlFOD5qsnEok"
      },
      "source": [
        "# -1 し、0 からラベルが始まるように変更\n",
        "t = t - 1\n",
        "np.unique(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Z7cti1nr_A"
      },
      "source": [
        "**学習用データセットとテスト用データセットの切り分け**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnCd5sA1oYGv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfpcstNgoYG6"
      },
      "source": [
        "x_train.shape, x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4ucQEFJoYHK"
      },
      "source": [
        "#### モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZhcDOTboYHL"
      },
      "source": [
        "import os, random\n",
        "\n",
        "def reset_seed(seed=0):\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97BwrInnoYHd"
      },
      "source": [
        "# 入力・出力層のノードの数を定義\n",
        "n_input, n_output = len(x_train[0]), len(np.unique(t_train))\n",
        "n_input, n_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGARg1PBqztq"
      },
      "source": [
        "使用するデータセットの入力変数同士のスケールが異なるため、バッチノーマリゼーション層を組み込み、ミニバッチごとのスケールを統一します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiXYpZc7oYHh"
      },
      "source": [
        "# シードの固定\n",
        "\n",
        "# モデルのインスタンス化\n",
        "\n",
        "\n",
        "# モデルの構築\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVbnTgQboYHs"
      },
      "source": [
        "#### モデルの可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8xuihxXoYH4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz2oICoroYIF"
      },
      "source": [
        "#### 目的関数・最適化手法の選択"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiXUKs39oYII"
      },
      "source": [
        "# モデルのコンパイル\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYj4dSA9oYIP"
      },
      "source": [
        "#### モデルの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B1olGABoYIR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpROSe1AoYIe"
      },
      "source": [
        "#### 予測精度の評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcPopD1qoYIf"
      },
      "source": [
        "# 学習結果を取得\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_yUAv1DoYIt"
      },
      "source": [
        "# 損失を可視化\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6Xp-VBZoMhQ"
      },
      "source": [
        "# 正解率を可視化\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUw3VdTsH5_"
      },
      "source": [
        "テスト用データセットに対する正解率が約 98% となっており、高い予測精度が確認できました。\n",
        "\n",
        "\n",
        "#### 学習済みモデルの保存と推論\n",
        "\n",
        "ファイルを `saved_model2` とし、学習済みモデルの保存を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieAPK7CisIeL"
      },
      "source": [
        "# 学習済みモデルの保存\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEPQFup9VKrP"
      },
      "source": [
        "---\n",
        "© 株式会社キカガク及び国立大学法人 豊橋技術科学大学"
      ]
    }
  ]
}